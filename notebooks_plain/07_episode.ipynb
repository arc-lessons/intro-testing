{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running tests with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- **Teaching:** 10 min\n",
    "- **Exercises:** 5 min\n",
    "\n",
    "**Questions**\n",
    "- How do I automate my tests?\n",
    "\n",
    "**Objectives**\n",
    "- Understand how to run a test suite using the pytest framework\n",
    "- Understand how to read the output of a pytest test suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We created a suite of tests for our mean function, but it was annoying to run them one at a time. It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed.\n",
    "\n",
    "Thankfully, that exists. Recall our tests are in a file called `test_mean.py`:\n",
    "\n",
    "```python\n",
    "from mean import *\n",
    "\n",
    "def test_ints():\n",
    "    num_list = [1,2,3,4,5]\n",
    "    obs = mean(num_list)\n",
    "    exp = 3\n",
    "    assert obs == exp\n",
    "\n",
    "def test_zero():\n",
    "    num_list=[0,2,4,6]\n",
    "    obs = mean(num_list)\n",
    "    exp = 3\n",
    "    assert obs == exp\n",
    "\n",
    "def test_double():\n",
    "    # This one will fail in Python 2\n",
    "    num_list=[1,2,3,4]\n",
    "    obs = mean(num_list)\n",
    "    exp = 2.5\n",
    "    assert obs == exp\n",
    "\n",
    "def test_long():\n",
    "    big = 100000000\n",
    "    obs = mean(range(1,big))\n",
    "    exp = big/2.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_complex():\n",
    "    # complex numbers are an unordered field\n",
    "    # however, this does NOT mean that \n",
    "    # the arithmetic mean of complex numbers is meaningless\n",
    "    num_list = [2 + 3j, 3 + 4j, -32 - 2j]\n",
    "    obs = mean(num_list)\n",
    "    exp = NotImplemented\n",
    "    assert obs == exp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The framework we will use to run our test suite is `pytest` which can be run from the directory containing the tests:\n",
    "```python\n",
    "! pytest\n",
    "```\n",
    "```brainfuck\n",
    "================================================ test session starts ================================================\n",
    "platform linux -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
    "rootdir: /home/rjg20/training/arc-training/intro-testing, inifile:\n",
    "collected 5 items\n",
    "\n",
    "test_mean.py ....F\n",
    "\n",
    "===================================================== FAILURES ======================================================\n",
    "___________________________________________________ test_complex ____________________________________________________\n",
    "\n",
    "    def test_complex():\n",
    "        # complex numbers are an unordered field\n",
    "        # however, this does NOT mean that \n",
    "        # the arithmetic mean of complex numbers is meaningless\n",
    "        num_list = [2 + 3j, 3 + 4j, -32 - 2j]\n",
    ">       obs = mean(num_list)\n",
    "\n",
    "test_mean.py:32:\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "sample = [(2+3j), (3+4j), (-32-2j)]\n",
    "\n",
    "    def mean(sample):\n",
    "        '''\n",
    "        Takes a list of numbers, sample\n",
    "\n",
    "        and returns the mean.\n",
    "        '''\n",
    "\n",
    "        assert len(sample) != 0, \"Unable to take the mean of an empty list\"\n",
    "        for value in sample:\n",
    ">           assert isinstance(value,int) or isinstance(value,float), \"Value in list is not a number.\"\n",
    "E           AssertionError: Value in list is not a number.\n",
    "\n",
    "mean.py:12: AssertionError\n",
    "======================================== 1 failed, 4 passed in 14.90 seconds ========================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did pytest do?\n",
    "\n",
    "In the above case, the pytest package ‘sniffed-out’ the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression `[Tt]est[-_]*`.\n",
    "\n",
    "The major boon a testing framework provides is exactly that, a utility to find and run the tests automatically. With pytest, this is the command-line tool called `pytest`. When `pytest` is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with `test`, import them, and run all of the functions and classes whose names start with `test` or `Test`. This automatic registration of test code saves tons of human time and allows us to focus on what is important: writing more tests (and the code to pass them!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you run `pytest`, it will print a dot (`.`) on the screen for every test that passes, an `F` for every test that fails or where there was an unexpected error. In rarer situations you may also see an `s` indicating a skipped tests (because the test is not applicable on your system) or a `x` for a known failure (because the developers could not fix it promptly). After the dots, `pytest` will print summary information.\n",
    "\n",
    "Without changing the tests, alter the `mean.py` file from the previous section until it passes. When it passes, `pytest` will produce results like the following:\n",
    "```python\n",
    "! pytest\n",
    "```\n",
    "```brainfuck\n",
    "collected 5 items\n",
    "\n",
    "test_mean.py .....\n",
    "\n",
    "========================== 5 passed in 2.68 seconds ===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also try running `pytest` with the *useful* flags `-s`, `-v`, and `-x`. Check the `pytest` manual to see what each of these do if you cannot determine it from their behaviour.\n",
    "\n",
    "As we write more code, we would write more tests, and `pytest` would produce more dots. Each passing test is a small, satisfying reward for having written quality scientific software. Now that you know how to write tests, let’s go into what can go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Points\n",
    "- The `pytest` command collects and runs tests starting with Test or test_.\n",
    "- `.` means the test passed\n",
    "- `F` means the test failed or errored\n",
    "- `x` is a known failure\n",
    "- `s` is a purposefully skipped test"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
